\section{Background}
\begin{enumerate}
    \item \textbf{Background of Goal Recognition},~\citet{meneguzzi2021survey} for general planning/goal recognition info, and~\citet{Scala2016,ScalaHaslum2016} for ENHSP.
    \item \textbf{Existing educational support systems and their limitations}, like how~\citet{Garrido2009} cannot monitor the learning routes, similar to intelligent tutoring systems;~\citet{Rojas2022} covers collaborative problem-solving and providing feedback in real-time, but does not necessarily provide an application to a classroom.
    \item \textbf{Overview of related work in the field of using AI and planning for educational assistance}~\cite{Castillo2009} summary.
    \item \textbf{The specific challenges faced by secondary education that necessitate innovative solutions}, diversity of learning styles and technological integration of learning solutions \textbf{TODO: Find Material}.
\end{enumerate}

\subsubsection{Theoretical Framework} 
In automated planning, the use of predicates and objects allow us to represent and reason about the world, what tasks may need to be carried out in that world, and what goals are achievable in the context of that world.
A predicate $\predicate\in\predicates$ is denoted by an n-ary predicate symbol, applied to a sequence of zero or more terms $\term_0,\dots,\term_n$. Terms are either constants or variables. We can refer to ground predicates, which represent logical values according to some interpretation as facts, which are divided into two types: positive and negated; as well as having constants for truth and false ($\top$ and $\bot$ respectively).
Now that we have defined a formal element for individual facts of the environment, we aggregate those which are relevant into a state.
A state $\states$ is a finite set of positive facts $\fact$ which follow the closed-world assumption. It follows that $\fact$ is true in $\states$ if $\fact\in\states$. We are also able to assume a simple inference relation; such that $\states\models\fact$ \textit{iff} $\fact\in\states,\states\not\models\fact$ \textit{iff} $\fact\not\in\states$, and $\states\models\bigwedge^{\size{\facts}}_{n=0}\fact_n$ \textit{iff} $\set{\fact_0,\dots,\fact_n}\supseteq\states$.
Planning domains describe the dynamics of an environment by employing operators characterised by a restricted first-order logical representation, creating schemata for actions aimed at altering the state. A schema is a template that offers a broad overview of task completion. It acts as a blueprint for a series of actions in different scenarios, allowing for the capture of common strategies for achieving goals. These schema templates promote knowledge reuse and adaptation, making problem-solving more efficient by tailoring abstract plans to the current context.
An operator $\action$ is represented through a tuple $\tuple{\name(\action),\pre(\action),\eff(\action)}$; where $\name(\action)$ represents the signature or description of $\action$; $\pre(\action)$ describes the preconditions of $\action$~and $\eff(\action)$ represents the effects of~$\action$. $\eff(\action)^+$ and $\eff(\action)^-$ divides $\eff(\action)$ into an add-list of positive predicates and a delete-list of negated predicates respectively. An action is a ground operator instantiated over its free variables. 
An action $\action$ is applicable to \states~if and only if $\states\models\pre(\action)$, and generates a new state $\states'$, such that $\states'\coloneqq(\states\cup\eff(\action)^+/\eff(\action)^-)$. 
A pair $\tuple{\facts,\actions}$ represents a planning domain definition $\planningdomain$, specifying the knowledge of the domain model, consisting of a finite set of facts and a finite set of actions (represented by $\facts$ and $\actions$ respectively).
Planning instances comprise both the planning domain and the planning problem, describing a finite set of objects of the environment. These are the initial state, and the goal state, which represent where the agent(s) would start and finish.
A planning instance $\planninginstance$ is represented by a tuple $\tuple{\planningdomain,\initial,\goals}$, whereby $\planningdomain=\tuple{\facts,\actions}$ represents the domain definition; $\initial\supseteq\facts$ is the initial state specification, defined through specifying the value for all facts in the initial state; and $\goals\supseteq\facts$ is the goal state specification, representing a singular goal, or a set of goals (that may be extremely large) satisfying the goal specification of the desired state to be achieved.
Classical planning representations often separate the definition of the initial state ($\initial$) and the goal state ($\goals$) as part of a planning problem ($\planningdomain$) to be utilised with a domain, such as STRIPS~\cite{Fikes1971} or PDDL~\cite{PDDL1998}.
For a planning instance $\planninginstance=\tuple{\planningdomain,\initial,\goals}$, a plan $\plan$ is a sequence of actions $\tuple{\action_0,\dots,\action_n}$ modifying the initial state $\initial$ into a state $\states\models\goals$, whereby the goal state $\goals$ holds by the successive execution of actions in a plan $\plan$. A plan is optimal ($\optimalplan$) with a length $\vert\optimalplan\vert$ if there exists no other plan $\plan'$ for $\planninginstance~\vert~ \plan'<\plan$.
Actions may have an associated cost applied to them. If an action does not have an associated cost, it is assumed to  have the unit cost ($1$) assigned to it. 
A plan ($\plan$) is optimal if its cost and respective length meets the evaluation criterion \textemdash~whether that's minimal, maximal, or otherwise.
A goal recognition task is a tuple $\recognitionproblem=\tuple{\planningdomain,\initial,\goals,\observationsequence}$; such that $\planningdomain=\tuple{\facts,\actions}$ is a planning domain definition; $\initial$ is the initial state; $\goals$ is the set of possible goals, also assuming the inclusion of some correct hidden goal $\hiddengoal$ ($\hiddengoal\in\goals$); and $\observationsequence=\tuple{\observation_0,\dots,\observation_n}$ is an observation sequence of executed actions. The observation sequence $\observationsequence$ is satisfied by a valid plan $\plan$, following that with each observation $\observation_i=\name(\action), \action\in\actions$, with $\action$ being part of $\plan$, and that plan $\plan$ transitioning $\initial$ into $\hiddengoal$ through the sequential execution of actions in $\plan$.
The optimal solution to a goal recognition task may consist of a singleton set that exclusively contains the correct hidden goal ($\hiddengoal\in\goals$) achieved by the observation sequence $\observationsequence$ of a plan execution.  In this context, we consider a solution suboptimal if it includes more than one goal, as all such goals are equally probable.
The solution to a goal recognition task $\recognitionproblem=\tuple{\planningdomain,\initial,\goals,\observationsequence}$ is a non-empty subset of the set of possible goals $\goalsolution\supseteq G~\vert~\forall G\in\goalsolution$, there exists a plan $\goalplan$ generated from a planning instance, and the observation sequence $\observationsequence$ is satisfied by $\goalplan$.